---
layout: post
permalink: /blog/why-you-do-this-kaggle
title: My first experience with Kaggle competitions
comments: true
---

<style>
  .center-image{
      margin: 0 auto;
      display: block;
  }
</style>

Sup ya'll. Been a long time since my last post. About a month I guess. Why? I don't know. Maybe it's because I'm lazy or maybe it's because I tend to procrastinate a lot or maybe it's an ugly combination of both. Anyway...

I'm doing a 5 month course on deep learning called [PadhAI](https://padhai.onefourthlabs.in). The end goal is to translate text in a picture to another language (Hindi to Tamil and vice versa). Seems pretty simple but it contains a LOT of steps. Kinda daunting at first but once you start understanding the concepts separately, you get a better idea of the big picture. In the first phase of the course, we were supposed to build a like/dislike predictor for mobile phones. The dataset given contained various feature like screen size, battery capacity, RAM... and much more. There were two contests, one using MP Neuron and Perceptron. 

After going over the initial hurdles of submitting the sample solution, I was able to successfully implement the MP Neuron model with an initial test categorization accuracy of about 77%. But this was just the basic model. I had not done any modifications to my model to make it more accurate. 

So for the next two days, I tried the following: 
* binarization of columns using the mean of each column
* binarization of columns using the mean of phones with a positive rating
* feature selection
* a lot more bullshit that I randomnly implemented without thinking and hoping it would boost accuracy. Spoiler alert: It didn't.

But to no avail. Frustrated, I tried other wacky solutions, and one seemed to work. I binarized each column based on the mean of phones with a negative rating. Seems counterintuitive but it worked. For the test data anyway. It boosted the accuracy from 77% to 82%. (this might not seem like a lot but in fact it is, in the world of machine learning). I was ecstatic. My public ranking went from somewhere around 900s, to the #110. I became the top 10% of that contest. Excited, I posted a WhatsApp status thinking I was Einstein or some shit. 

![whyidodat](/images/kaggle-yay.jpg "Woohoo!"){: .center-image .img-responsive}

Later on, I figured out that binarizing by the mean of disliked phones is not the right thing to do, and it just managed to work for the sample test data by luck and will not help when it comes to the training data. So, being the scaredycat I am, changed my code to binarize the columns using the mean of each column of the liked phones. Accuracy dropped back to 77%, but I was willing to take my chances. For those who don't know, the leaderboard is split into public leaderboard and private leaderboard. Public leaderboards uses 30% of the test data but the private leaderboard uses 70% of the test data.)

To my utter dismay, it did not work. I woke up the next day, expected a top 10% score on the leaderboard but no. My rank had shot down *985 places*. I felt so disheartened I didn't look at the course for an entire week, lol.

![i cri everytiem](/images/kaggle-sad.png "I want to jump off a cliff."){: .center-image .img-responsive}

Thinking back, there might be a few reasons why my rank decreased: 
* Wrong implementation of mean binarization
* Failure to do feature selection. Some columns could not have had any effect on the rating of the phones, and dropping them would've increased the accuracy. 
* Selective binarization on each column. I still don't know how this works. 
* Kaggle sucks. Everytime you submit your model, you cross your fingers and hope that you get a good accuracy. Looking at the leaderboards, it looks like the scoring is based on luck for the most part. I've seen people who have shot up 900 places and people have shot down 900 places. Unfortunately I was among the latter.

I mean, look at this guy: 

![impossibru](/images/kaggle-omg.png "WHY. JUST WHY?"){: .center-image .img-responsive}
![wtf](https://media.giphy.com/media/aZ3LDBs1ExsE8/giphy.gif){: .center-image}

Here's a quora comment that captures my experience: 

> There is an art and a science in doing so and experience Kagglers are able to make the most out of it without overfitting. If not done well, that usually lends itself to worse scores on the private LB, sometimes disasters. In general the key is to build a model that generalizes well. Local cross validation is essential for that. But the public LB provides also some signal. Kaggle Masters can use cross validation and public LB to optimize private LB scores.

*I think I get the gist of how Kaggle works. I don't like it, but I kinda like it.*

![wat](https://media.giphy.com/media/pPhyAv5t9V8djyRFJH/giphy.gif){: .center-image}

### Source Code

Meh. I don't like how I coded it. I'm going to post it anyway. My solution is at the bottom section of the notebook. 

{% gist 78dd5f64e991e8f22a9e8e77012e3e91 %}

By the time I wrote this, I've finished 2 other contests. I will write another blog post when something exciting happens. 

*Till then, it's sayonara.*

From the stars, 

FR.
